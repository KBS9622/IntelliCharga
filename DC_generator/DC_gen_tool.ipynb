{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from sympy import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit, least_squares\n",
    "from scipy.optimize import minimize as sp_minimize\n",
    "from scipy import special\n",
    "from lmfit import minimize, Minimizer, Parameters, Parameter, report_fit\n",
    "import matplotlib.pyplot as plt\n",
    "import lmfit\n",
    "import random\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(file_name, subdir=''):\n",
    "    \"\"\"\n",
    "    Loads data from .csv file in to DataFrame\n",
    "\n",
    "    :param file_name: .csv file name in string\n",
    "    :param subdir: optional parameter to specify the subdirectory of the file\n",
    "    :return: extracted data in DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    file_dir = os.path.realpath('../')\n",
    "    print(file_dir)\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        if root.endswith(subdir):\n",
    "            for name in files:\n",
    "                if name == file_name:\n",
    "                    file_path = os.path.join(root, name)\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LF_Noise(component = 1):\n",
    "    \"\"\" Inital parameters and bounds for each paramter according to LF (0-0.01Hz)in the paper\n",
    "    \"\"\"\n",
    "    LMparams = Parameters()\n",
    "\n",
    "    # The code below is to load the initial paramters if we are running the NLLSR on all 3 components at once\n",
    "\n",
    "    LMparams.add('A1_FS', value = 10.)\n",
    "    LMparams.add('A2_FS', value = 10.)\n",
    "    LMparams.add('A3_FS', value = 10.)\n",
    "    LMparams.add('w1_FS', value = 0, min = 0, max = 0.01*2*math.pi)\n",
    "    LMparams.add('w2_FS', value = 0.005*2*math.pi, min = 0, max = 0.01*2*math.pi)\n",
    "    LMparams.add('w3_FS', value = 0.01*2*math.pi, min = 0, max = 0.01*2*math.pi)\n",
    "    LMparams.add('phi1_FS', value = 0, min = -math.pi, max = math.pi)\n",
    "    LMparams.add('phi2_FS', value = 0, min = -math.pi, max = math.pi)\n",
    "    LMparams.add('phi3_FS', value = 0, min = -math.pi, max = math.pi)\n",
    "\n",
    "    # The code below is to load the initial paramters if we are running the NLLSR on one component at a time\n",
    "\n",
    "    # if component == 1:\n",
    "    #     LMparams.add('A_FS', value = 10.)\n",
    "    #     LMparams.add('w_FS', value = 0, min = 0, max = 0.01*2*math.pi)\n",
    "    #     LMparams.add('phi_FS', value = 0, min = -math.pi, max = math.pi)\n",
    "    # elif component == 2:\n",
    "    #     LMparams.add('A_FS', value = 10.)\n",
    "    #     LMparams.add('w_FS', value = 0.005*2*math.pi, min = 0, max = 0.01*2*math.pi)\n",
    "    #     LMparams.add('phi_FS', value = 0, min = -math.pi, max = math.pi)\n",
    "    # elif component == 3:\n",
    "    #     LMparams.add('A_FS', value = 10.)\n",
    "    #     LMparams.add('w_FS', value = 0.01*2*math.pi, min = 0, max = 0.01*2*math.pi)\n",
    "    #     LMparams.add('phi_FS', value = 0, min = -math.pi, max = math.pi)\n",
    "\n",
    "    return LMparams\n",
    "\n",
    "def MF_Noise(component = 1):\n",
    "    \"\"\" Inital parameters and bounds for each paramter according to MF (0.01-0.25Hz) in the paper\n",
    "    \"\"\"\n",
    "    LMparams = Parameters()\n",
    "\n",
    "    # The code below is to load the initial paramters if we are running the NLLSR on all 3 components at once\n",
    "\n",
    "    LMparams.add('A1_FS', value = 10.)\n",
    "    LMparams.add('A2_FS', value = 10.)\n",
    "    LMparams.add('A3_FS', value = 10.)\n",
    "    LMparams.add('w1_FS', value = 0.02*2*math.pi, min = 0.01*2*math.pi, max = 0.25*2*math.pi)\n",
    "    LMparams.add('w2_FS', value = 0.03*2*math.pi, min = 0.01*2*math.pi, max = 0.25*2*math.pi)\n",
    "    LMparams.add('w3_FS', value = 0.03*2*math.pi, min = 0.01*2*math.pi, max = 0.25*2*math.pi)\n",
    "    LMparams.add('phi1_FS', value = 0, min = -math.pi, max = math.pi)\n",
    "    LMparams.add('phi2_FS', value = 0, min = -math.pi, max = math.pi)\n",
    "    LMparams.add('phi3_FS', value = 0, min = -math.pi, max = math.pi)\n",
    "\n",
    "    # The code below is to load the initial paramters if we are running the NLLSR on one component at a time\n",
    "\n",
    "    # if component == 1:\n",
    "    #     LMparams.add('A_FS', value = 10.)\n",
    "    #     LMparams.add('w_FS', value = 0.02*2*math.pi, min = 0.01*2*math.pi, max = 0.25*2*math.pi)\n",
    "    #     LMparams.add('phi_FS', value = 0, min = -math.pi, max = math.pi)\n",
    "    # elif component == 2:\n",
    "    #     LMparams.add('A_FS', value = 10.)\n",
    "    #     LMparams.add('w_FS', value = 0.03*2*math.pi, min = 0.01*2*math.pi, max = 0.25*2*math.pi)\n",
    "    #     LMparams.add('phi_FS', value = 0, min = -math.pi, max = math.pi)\n",
    "    # elif component == 3:\n",
    "    #     LMparams.add('A_FS', value = 10.)\n",
    "    #     LMparams.add('w_FS', value = 0.03*2*math.pi, min = 0.01*2*math.pi, max = 0.25*2*math.pi)\n",
    "    #     LMparams.add('phi_FS', value = 0, min = -math.pi, max = math.pi)\n",
    "\n",
    "    return LMparams\n",
    "\n",
    "def HF_Noise(component = 1):\n",
    "    \"\"\" Inital parameters and bounds for each paramter according to HF (0.25-0.5Hz) in the paper\n",
    "    \"\"\"\n",
    "    LMparams = Parameters()\n",
    "\n",
    "    # The code below is to load the initial paramters if we are running the NLLSR on all 3 components at once\n",
    "\n",
    "    LMparams.add('A1_FS', value = 1.)\n",
    "    LMparams.add('A2_FS', value = 1.)\n",
    "    LMparams.add('A3_FS', value = 1.)\n",
    "    LMparams.add('w1_FS', value = 0.25*2*math.pi, min = 0.25*2*math.pi, max = 0.5*2*math.pi)\n",
    "    LMparams.add('w2_FS', value = 0.375*2*math.pi, min = 0.25*2*math.pi, max = 0.5*2*math.pi)\n",
    "    LMparams.add('w3_FS', value = 0.5*2*math.pi, min = 0.25*2*math.pi, max = 0.5*2*math.pi)\n",
    "    LMparams.add('phi1_FS', value = 0, min = -math.pi, max = math.pi)\n",
    "    LMparams.add('phi2_FS', value = 0, min = -math.pi, max = math.pi)\n",
    "    LMparams.add('phi3_FS', value = 0, min = -math.pi, max = math.pi)\n",
    "\n",
    "    # The code below is to load the initial paramters if we are running the NLLSR on one component at a time\n",
    "\n",
    "    # if component == 1:\n",
    "    #     LMparams.add('A_FS', value = 1.)\n",
    "    #     LMparams.add('w_FS', value = 0.25*2*math.pi, min = 0.25*2*math.pi, max = 0.5*2*math.pi)\n",
    "    #     LMparams.add('phi_FS', value = 0, min = -math.pi, max = math.pi)\n",
    "    # elif component == 2:\n",
    "    #     LMparams.add('A_FS', value = 1.)\n",
    "    #     LMparams.add('w_FS', value = 0.375*2*math.pi, min = 0.25*2*math.pi, max = 0.5*2*math.pi)\n",
    "    #     LMparams.add('phi_FS', value = 0, min = -math.pi, max = math.pi)\n",
    "    # elif component == 3:\n",
    "    #     LMparams.add('A_FS', value = 1.)\n",
    "    #     LMparams.add('w_FS', value = 0.5*2*math.pi, min = 0.25*2*math.pi, max = 0.5*2*math.pi)\n",
    "    #     LMparams.add('phi_FS', value = 0, min = -math.pi, max = math.pi)\n",
    "\n",
    "    return LMparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "random_no = random.random()\n",
    "print(round(random_no,3))\n",
    "random_no = random.random()\n",
    "print(random_no)\n",
    "\n",
    "np.random.seed(10)\n",
    "list1 = np.random.rand(5).tolist()\n",
    "random_numbers = [round(element, 2) for element in list1]\n",
    "print(random_numbers.pop())\n",
    "print(random_numbers.pop())\n",
    "print(random_numbers.pop())\n",
    "print(random_numbers.pop())\n",
    "print(random_numbers.pop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DP(object):\n",
    "    \"\"\" Module 3: Drive Pulse\n",
    "    \"\"\"\n",
    "    # def __init__(self, pulse_duration):\n",
    "    #     \"\"\" Constructor takes in pulse duration and creates instances of inverse cdfs for each of the intersted parameters needed to\n",
    "    #         generate the drive pulse. Creates self.parameter dictionary containing all relevant parameters.\n",
    "    #     \"\"\"\n",
    "    #     self.pulse_duration = pulse_duration \n",
    "    #     self.total_t = np.linspace(0, 1000*self.pulse_duration, (1000*self.pulse_duration)+1)\n",
    "    #     self.accel_inv_cdf_obj = self.create_inv_cdf_objects(Acceleration())\n",
    "    #     self.cruising_duration_inv_cdf_obj = self.create_inv_cdf_objects(Cruising_Duration())\n",
    "    #     self.avg_cruising_speed_inv_cdf_obj = self.create_inv_cdf_objects(Average_Crusing_Speed())\n",
    "    #     self.decel_inv_cdf_obj = self.create_inv_cdf_objects(Deceleration())\n",
    "    #     self.random_select_params()\n",
    "    #     self.params = self.parameters_for_drive_cycle(self.accel_value, self.decel_value, self.cruising_duration_value, self.avg_cruising_speed_value)\n",
    "    #     print(self.params)\n",
    "\n",
    "    def __init__(self, pulse_duration, extract_obj):\n",
    "        \"\"\" Constructor takes in pulse duration and extraction obj, then creates instances of inverse cdfs for each of the interested \n",
    "        parameters needed to generate the drive pulse. Creates self.parameter dictionary containing all relevant parameters.\n",
    "        \"\"\"\n",
    "        self.pulse_duration = pulse_duration \n",
    "        self.total_t = np.linspace(0, 1000*self.pulse_duration, (1000*self.pulse_duration)+1)\n",
    "        self.accel_inv_cdf_obj = self.create_inv_cdf_objects(extract_obj.accel_obj)\n",
    "        self.cruising_duration_inv_cdf_obj = self.create_inv_cdf_objects(extract_obj.cd_obj)\n",
    "        self.avg_cruising_speed_inv_cdf_obj = self.create_inv_cdf_objects(extract_obj.avg_cs_obj)\n",
    "        self.decel_inv_cdf_obj = self.create_inv_cdf_objects(extract_obj.decel_obj)\n",
    "        self.random_select_params()\n",
    "        self.params = self.parameters_for_drive_cycle(self.accel_value, self.decel_value, self.cruising_duration_value, self.avg_cruising_speed_value)\n",
    "        print(self.params)\n",
    "\n",
    "\n",
    "    def create_inv_cdf_objects(self, attribute_obj, num_of_gauss = 2):\n",
    "        \"\"\"\n",
    "        Takes in one of parameter's classes needed for driving pulse and instantiates object for the inverse_cdf\n",
    "        \"\"\"\n",
    "        #load initial gaussian parameters\n",
    "        param_obj = Gaussian_param()\n",
    "        #create a probability function object for attribute with its attribute histogram data\n",
    "        attribute_prob_obj = Probability_Functions(attribute_obj.bins, attribute_obj.data_points,num_of_gauss)\n",
    "        #fit the histogram\n",
    "        fitted_obj = attribute_prob_obj.NLLSR(param_obj)\n",
    "        # create inverse cdf object\n",
    "        inv_cdf_obj = inv_cdf(attribute_prob_obj)\n",
    "\n",
    "        return inv_cdf_obj\n",
    "\n",
    "    def random_select_params(self, seed = 10):\n",
    "        \"\"\"\n",
    "        Randomly generates numbers from 0 to 1 to randomly select values for parameters using their inverse cdf\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        list1 = np.random.rand(4).tolist()\n",
    "        random_numbers = [round(element, 2) for element in list1]\n",
    "        self.accel_value = self.accel_inv_cdf_obj.get_value(random_numbers.pop())[0]\n",
    "        # self.cruising_duration_value = self.cruising_duration_inv_cdf_obj.get_value(random_numbers.pop())[0]\n",
    "        self.cruising_duration_value = self.cruising_duration_inv_cdf_obj.get_value(0.98)[0]\n",
    "        self.avg_cruising_speed_value = self.avg_cruising_speed_inv_cdf_obj.get_value(random_numbers.pop())[0]\n",
    "        self.decel_value = self.decel_inv_cdf_obj.get_value(random_numbers.pop())[0]\n",
    "\n",
    "    def crusing_with_noise(self, time_array, velocity_noise_obj):\n",
    "        \"\"\"\n",
    "        time_array: array of timestamps to compute for corresponding velocity noise using fitted model\n",
    "        velocity_noise_obj: VN object containing fitted model and respective parameters for 3 freq components\n",
    "        \n",
    "        return:: cruising_with_noise set of speed values containing average cruising speed superimposed on velocity noise \n",
    "        \"\"\"\n",
    "        #resets time to match the specified duration of the driving pulse\n",
    "        velocity_noise_obj.set_t(time_array)\n",
    "        #returns velocity noise speed values (y axis)\n",
    "        velocity_noise = velocity_noise_obj.final_curve()\n",
    "        #adds velocity noise speed values (y axis) to static cruising speed\n",
    "        cruising_with_noise =  velocity_noise + self.params[\"cruising speed\"]\n",
    "\n",
    "        plt.plot(time_array, cruising_with_noise)\n",
    "        plt.show()\n",
    "        return cruising_with_noise\n",
    "\n",
    "    def parameters_for_drive_cycle(self, acceleration, decceleration, cruising_duration, average_cruising_speed):\n",
    "        \"\"\"\n",
    "        accepts as parameters the 4 randomly selected values from the inverse cdfs, computes other parameters and returns\n",
    "        this as a dictionary\n",
    "        \"\"\"\n",
    "        #computes acceleration time by using average cruising speed as initial speed of cruising duration\n",
    "        acceleration_time= 1000 * average_cruising_speed / acceleration\n",
    "        #computes decceleration time by using average cruising speed as final speed of cruising duration\n",
    "        decceleration_time = 1000 * average_cruising_speed / decceleration\n",
    "        #computes idle time by subtracting all other durations from total pulse duration\n",
    "        idle_time = self.total_t - acceleration_time - decceleration_time - (1000 * cruising_duration)\n",
    "        \n",
    "        #constructs parameters dictionary containing everything needed to generate driving pulse\n",
    "        parameters = {\"acceleration\": acceleration,\n",
    "                      \"decceleration\" : decceleration,\n",
    "                      \"acceleration duration\": round(acceleration_time),\n",
    "                      \"decceleration duration\": round(decceleration_time),\n",
    "                      \"cruising duration\" : round(1000 * cruising_duration),\n",
    "                      \"cruising speed\" : average_cruising_speed,\n",
    "                      \"idle duration\" : idle_time,\n",
    "                      \"total duration\": self.total_t\n",
    "                      }\n",
    "        return parameters\n",
    "\n",
    "    def generate_drive_cycle(self, velocity_noise_obj):\n",
    "        \"\"\" Function to be called from outside the class that outputs plot of generated driving pulse\n",
    "        \"\"\"\n",
    "        # Call cruising_with_noise method to return corresponding values (y axis) for cruising\n",
    "        # Inputs the whole pulse duration as cruise duration therefore extra values are present\n",
    "        # print(self.total_t)\n",
    "        # print(self.params[\"acceleration duration\"])\n",
    "        # print(np.where(self.total_t[:]==self.params[\"acceleration duration\"]))\n",
    "        # print(np.where(self.total_t[:]==self.params[\"acceleration duration\"])[0][0])\n",
    "        # print(self.total_t[np.where(self.total_t[:]==self.params[\"acceleration duration\"])[0][0]:])\n",
    "        speed_while_cruising_extra_values = self.crusing_with_noise(self.total_t[np.where(self.total_t[:]==self.params[\"acceleration duration\"])[0][0]:]/1000, velocity_noise_obj)\n",
    "        # computes initial cruising speed with velocity noise\n",
    "        initial_cruising_speed = speed_while_cruising_extra_values[0]\n",
    "        # computes actual acceleartion duration using the caluclated initial speed\n",
    "        # no longer is using the estimate of initial= avergae cruising speed as in the parameters_for_drive_cycle method\n",
    "        self.params[\"acceleration duration\"] = round(1000 * initial_cruising_speed / self.params[\"acceleration\"])\n",
    "        # print('accel_duration:{}'.format(self.params[\"acceleration duration\"]))\n",
    "        # print(self.total_t)\n",
    "        # print(1000*self.params[\"acceleration duration\"])\n",
    "        # print(np.where(self.total_t[:]==self.params[\"acceleration duration\"]))\n",
    "        \n",
    "        # Retrieves x axis (time steps) and y axis (speed) values during acceleration period\n",
    "        accel_time_values = self.total_t[:np.where(self.total_t[:]==self.params[\"acceleration duration\"])[0][0]]\n",
    "        speed_during_acceleration = self.params[\"acceleration\"] * accel_time_values / 1000\n",
    "        current_time = self.params[\"acceleration duration\"]\n",
    "        \n",
    "        # Retrieves x axis (time steps) and y axis (speed) values during cruising period\n",
    "        cruising_time_values = self.total_t[np.where(self.total_t[:]==current_time)[0][0]:np.where(self.total_t[:]==current_time+self.params[\"cruising duration\"])[0][0]]\n",
    "        speed_during_cruising = speed_while_cruising_extra_values[:np.where(self.total_t[:]==self.params[\"cruising duration\"])[0][0]]\n",
    "        current_time += self.params[\"cruising duration\"]\n",
    "        \n",
    "        # Retrieves x axis (time steps) and y axis (speed) values during decceleration period\n",
    "        final_cruising_speed = speed_during_cruising[-1]\n",
    "        # print(final_cruising_speed)\n",
    "        # print(self.params[\"decceleration\"])\n",
    "        self.params[\"decceleration duration\"] = round(1000 * final_cruising_speed / self.params[\"decceleration\"])\n",
    "        # print(self.params[\"decceleration duration\"])\n",
    "        end_time = round(current_time+self.params[\"decceleration duration\"])\n",
    "        deccel_time_values = self.total_t[np.where(self.total_t[:]==current_time)[0][0]:np.where(self.total_t[:]==end_time)[0][0]]\n",
    "        speed_during_decceleration = final_cruising_speed - (self.params[\"decceleration\"]/1000 * np.linspace(1, self.params[\"decceleration duration\"], int(self.params[\"decceleration duration\"])))\n",
    "        current_time += self.params[\"decceleration duration\"] \n",
    "        # current_time = round(current_time,3)\n",
    "        \n",
    "        # Retrieves x axis (time steps) and y axis (speed= 0) values during idle_time period\n",
    "        idle_time_values = self.total_t[np.where(self.total_t[:]==current_time)[0][0]:np.where(self.total_t[:]==self.pulse_duration)[0][0]]                                            \n",
    "        idle_time = 0 * idle_time_values\n",
    "        \n",
    "        # Plot all 4 periods on same plot to visuale driving pulse\n",
    "        plt.plot(accel_time_values, speed_during_acceleration, 'r')  \n",
    "        plt.plot(cruising_time_values, speed_during_cruising, 'b') \n",
    "        plt.plot(deccel_time_values, speed_during_decceleration, 'g') \n",
    "        plt.plot(idle_time_values, idle_time, 'r')\n",
    "        plt.show()\n",
    "\n",
    "        time_steps = np.concatenate((accel_time_values,cruising_time_values,deccel_time_values,idle_time_values))\n",
    "        speed = np.concatenate((speed_during_acceleration,speed_during_cruising,speed_during_decceleration,idle_time))\n",
    "\n",
    "        df = pd.DataFrame({'time_steps':time_steps, 'speed':speed}) \n",
    "\n",
    "        df.to_csv('generated_data.csv', index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(object):\n",
    "    \"\"\" Module 2: Drive Scenario\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DC(object):\n",
    "    \"\"\" Module 1: Drive Cycle\n",
    "    \"\"\"\n",
    "    def __init__(self, dc_length):\n",
    "        \n",
    "        t_DC = dc_length\n",
    "        sum_t_DS = 0\n",
    "        while t_DC > sum_t_DS:\n",
    "            pass\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Acceleration(object):\n",
    "    def __init__(self):\n",
    "        # histogram from paper\n",
    "        self.bins = pd.DataFrame(np.linspace(0.1, 2.098, num=1000))\n",
    "        self.data_points = pd.DataFrame(np.repeat([1,4,12,24,20,29,9,1,0,0],100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cruising_Duration(object):\n",
    "    def __init__(self):\n",
    "        # histogram from paper\n",
    "        self.bins = pd.DataFrame(np.linspace(-6, 125.88, num=1100))\n",
    "        self.data_points = pd.DataFrame(np.repeat([3,14,28,26,12,7,2,4,3,0,1],100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Average_Crusing_Speed(object):\n",
    "    def __init__(self):\n",
    "        # histogram from paper\n",
    "        self.bins = pd.DataFrame(np.linspace(1, 20.98, num=1000))\n",
    "        self.data_points = pd.DataFrame(np.repeat([0,1,2,6,18,35,22,11,4,1],100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deceleration(object):\n",
    "    def __init__(self):\n",
    "        # histogram from paper\n",
    "        self.bins = pd.DataFrame(np.linspace(-0.25, 4.745, num=1000))\n",
    "        self.data_points = pd.DataFrame(np.repeat([0,0,6,14,22,20,16,13,6,3],100))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attribute(object):\n",
    "    \"\"\" Generates a histogram according to the input array\n",
    "    \"\"\"\n",
    "    def __init__(self, array):\n",
    "        \"\"\" Takes in a numpy array and generates a histogram then repeat the datapoints to get a smooth and consistent graph\n",
    "        Zero frequency bins are added to both side of the histogram to help Gaussian fitting\n",
    "        \"\"\"\n",
    "        self.array = array\n",
    "        # generates a histogram from the input numpy array\n",
    "        hist, bin_edges = np.histogram(array)\n",
    "        # get the bin width of the histogram\n",
    "        bin_width = bin_edges[1]-bin_edges[0]\n",
    "        # calculate the number of bins in the histogram\n",
    "        num_of_bins = len(bin_edges)-1\n",
    "        # how granular do we want the datapoints to be\n",
    "        repeat_factor = 100\n",
    "        # zero bins on both side of histogram to help gaussian fit better\n",
    "        num_of_zero_bins = 4 # on both sides total\n",
    "        # new total number of bins in the histogram including zero bins\n",
    "        new_num_of_bins = num_of_bins + num_of_zero_bins\n",
    "        # zero frequency arrays for the zero bins\n",
    "        zero_array = np.repeat([0],num_of_zero_bins/2)\n",
    "        # concatenate zero arrays and hist to form new hist\n",
    "        new_hist = np.concatenate((zero_array,hist,zero_array))\n",
    "        # new first bin edge\n",
    "        new_first_bin_edge = bin_edges[0]-((num_of_zero_bins/2)*bin_width)\n",
    "        # new last bin edge\n",
    "        new_last_bin_edge = bin_edges[-1]+((num_of_zero_bins/2)*bin_width)\n",
    "        # generate fine 'bins' based on inital bin numbers and repeat factor\n",
    "        self.bins = pd.DataFrame(np.linspace(new_first_bin_edge, new_last_bin_edge-(bin_width/repeat_factor), num=repeat_factor*new_num_of_bins))\n",
    "        # repeat the 'frequency' values by the repeat factor\n",
    "        self.data_points = pd.DataFrame(np.repeat(new_hist,repeat_factor))\n",
    "\n",
    "class Extract_Hist(object):\n",
    "    \"\"\" This object handles all the extraction of information from labelled data and generates histogram objects based on the data\n",
    "    \"\"\"\n",
    "    def __init__(self, file_name, subdir=''):\n",
    "        \"\"\" Constructor takes in the file name and subdirectory to locate the file and save it as a pandas df\n",
    "        Then subsets the data by labelled data and completely labelled data and extract relevant attribute information to form histograms\n",
    "        \"\"\"\n",
    "        self.df = load_csv_data(file_name=file_name,subdir=subdir)\n",
    "        self.df['timestamp'] = pd.to_datetime(self.df['timestamp'], dayfirst=True)\n",
    "        # identify the labelled points in the data\n",
    "        self.identify_points()\n",
    "        # extract all the required attribute information and save the histogram as a local variable\n",
    "        self.extract_acceleration()\n",
    "        self.extract_cruise_duration()\n",
    "        self.extract_avg_cruise_speed()\n",
    "        self.extract_deceleration()\n",
    "        self.extract_idle_duration()\n",
    "    \n",
    "    def identify_points(self):\n",
    "        \"\"\" Indentifies and subsets the labelled observations in the data and then subset to a smaller df with completely labelled pulses\n",
    "        Completely labelled pulses are pulses where the label points are in the order '1,2,3,4' and is not missing any values in between\n",
    "        \"\"\"\n",
    "        self.points = self.df.copy()\n",
    "        # only get the observations for which the points are labeled\n",
    "        self.points = self.points[self.points['points']>0]\n",
    "        # reset the index\n",
    "        self.points.reset_index(inplace=True)\n",
    "        #print out the values of points to visually determine if there are any errors in labelling order\n",
    "        # print(self.points['points'].to_numpy())\n",
    "        # create a list to store only completely labelled pulses\n",
    "        self.complete_labels = pd.DataFrame(columns=self.points.columns)\n",
    "        # print(self.complete_labels)\n",
    "        # identify which observations are not fully labeled (1 and 4 labelled but not 2 and 3)\n",
    "        i = 0\n",
    "        while True:\n",
    "            if i+3 > len(self.points):\n",
    "                # Condition to break out of the while loop when entire df is gone through, or the last pulse is incomplete\n",
    "                break\n",
    "            # create dummy variable thats a slice of the whole data\n",
    "            temp = self.points.loc[i:i+3]\n",
    "            if temp.loc[i,'points'] == 1:\n",
    "                if temp.loc[i+1,'points'] == 2:\n",
    "                    if temp.loc[i+2,'points'] == 3:\n",
    "                        if temp.loc[i+3,'points'] == 4:\n",
    "                            # this means the entire slice is correctly ordered and nothing is missing, \n",
    "                            # therefore append to the complete_labels df\n",
    "                            self.complete_labels = self.complete_labels.append(temp, ignore_index=True)\n",
    "                            # shift index by four for the next 4 values\n",
    "                            i += 4\n",
    "                            # skip to the next iteration of loop\n",
    "                            continue\n",
    "            # skip the first observation of the temp df because the four consecutive values do not line up\n",
    "            i += 1\n",
    "            # prints incomplete when there is an incompletely labelled pulse\n",
    "            print('incomplete')\n",
    "\n",
    "    def extract_acceleration(self):\n",
    "        \"\"\" Extracts acceleration data and generates a histogram object to store the data\n",
    "        \"\"\"\n",
    "        # get the timestamp and speed for points = 1 (start of acceleration phase)\n",
    "        accel_start = self.complete_labels[self.complete_labels['points']==1]\n",
    "        accel_start = accel_start.loc[:,['timestamp','speed_mps']]\n",
    "        # reset the index\n",
    "        accel_start.reset_index(drop = True, inplace=True)\n",
    "        # get the timestamp and speed for points = 2 (end of acceleration phase)\n",
    "        accel_end = self.complete_labels[self.complete_labels['points']==2]\n",
    "        accel_end = accel_end.loc[:,['timestamp','speed_mps']]\n",
    "        # reset the index\n",
    "        accel_end.reset_index(drop = True, inplace=True)\n",
    "        \n",
    "        # empty list to store acceleration values\n",
    "        accel_values = []\n",
    "        for i in range(len(accel_start)):\n",
    "            # calculate time difference\n",
    "            time_diff = (accel_end.loc[i,'timestamp'] - accel_start.loc[i,'timestamp']).seconds\n",
    "            # calculate speed difference\n",
    "            speed_diff = accel_end.loc[i,'speed_mps'] - accel_start.loc[i,'speed_mps']\n",
    "            # calculate acceleration value\n",
    "            accel = speed_diff/time_diff\n",
    "            # append the values\n",
    "            accel_values.append(accel)\n",
    "        # generates and saves the acceleration histogram object\n",
    "        self.accel_obj = Attribute(np.array(accel_values))\n",
    "\n",
    "        return self.accel_obj\n",
    "\n",
    "    def extract_deceleration(self):\n",
    "        \"\"\" Extracts deceleration data and generates a histogram object to store the data\n",
    "        \"\"\"\n",
    "        # get the timestamp and speed for points = 3 (start of deceleration phase)\n",
    "        decel_start = self.complete_labels[self.complete_labels['points']==3]\n",
    "        decel_start = decel_start.loc[:,['timestamp','speed_mps']]\n",
    "        # reset the index\n",
    "        decel_start.reset_index(drop = True, inplace=True)\n",
    "        # get the timestamp and speed for points = 4 (end of deceleration phase)\n",
    "        decel_end = self.complete_labels[self.complete_labels['points']==4]\n",
    "        decel_end = decel_end.loc[:,['timestamp','speed_mps']]\n",
    "        # reset the index\n",
    "        decel_end.reset_index(drop = True, inplace=True)\n",
    "        \n",
    "        # empty list to store deceleration values\n",
    "        decel_values = []\n",
    "        for i in range(len(decel_start)):\n",
    "            # calculate time difference\n",
    "            time_diff = (decel_end.loc[i,'timestamp'] - decel_start.loc[i,'timestamp']).seconds\n",
    "            # calculate speed difference\n",
    "            speed_diff = decel_end.loc[i,'speed_mps'] - decel_start.loc[i,'speed_mps']\n",
    "            # calculate deceleration value (needs to be positive for rest of code)\n",
    "            decel = -(speed_diff/time_diff)\n",
    "            # append the values\n",
    "            decel_values.append(decel)\n",
    "        # generates and saves the deceleration histogram object\n",
    "        self.decel_obj = Attribute(np.array(decel_values))\n",
    "\n",
    "        return self.decel_obj\n",
    "\n",
    "    def extract_cruise_duration(self):\n",
    "        \"\"\" Extracts cruising duration data and generates a histogram object to store the data\n",
    "        \"\"\"\n",
    "        # get the timestamp for points = 2 (start of cruise phase)\n",
    "        cruise_start = self.complete_labels[self.complete_labels['points']==2]\n",
    "        cruise_start = cruise_start.loc[:,['timestamp']]\n",
    "        # reset the index\n",
    "        cruise_start.reset_index(drop = True, inplace=True)\n",
    "        # get the timestamp for points = 3 (end of cruise phase)\n",
    "        cruise_end = self.complete_labels[self.complete_labels['points']==3]\n",
    "        cruise_end = cruise_end.loc[:,['timestamp']]\n",
    "        # reset the index\n",
    "        cruise_end.reset_index(drop = True, inplace=True)\n",
    "\n",
    "        # empty list to store cruising duration values\n",
    "        cruise_duration_values = []\n",
    "        for i in range(len(cruise_start)):\n",
    "            # calculate time difference\n",
    "            cruise_duration = (cruise_end.loc[i,'timestamp'] - cruise_start.loc[i,'timestamp']).seconds\n",
    "            # append the values\n",
    "            cruise_duration_values.append(cruise_duration)\n",
    "        # generates and saves the crusing duration histogram object\n",
    "        self.cd_obj = Attribute(np.array(cruise_duration_values))\n",
    "\n",
    "        return self.cd_obj\n",
    "\n",
    "    def extract_avg_cruise_speed(self):\n",
    "        \"\"\" Extracts average crusing speed data and generates a histogram object to store the data\n",
    "        Extracts the cruising phase with velocity noise data and saves it\n",
    "        \"\"\"\n",
    "        # get the timestamp and speed for points = 2 (start of cruise phase)\n",
    "        cruise_start = self.complete_labels[self.complete_labels['points']==2]\n",
    "        cruise_start = cruise_start.loc[:,['index']]\n",
    "        # reset the index\n",
    "        cruise_start.reset_index(drop = True, inplace=True)\n",
    "        # get the timestamp and speed for points = 3 (end of cruise phase)\n",
    "        cruise_end = self.complete_labels[self.complete_labels['points']==3]\n",
    "        cruise_end = cruise_end.loc[:,['index']]\n",
    "        # reset the index\n",
    "        cruise_end.reset_index(drop = True, inplace=True)\n",
    "\n",
    "        # empty list to store crusing phase with velocity noise data\n",
    "        self.cruise_with_vn = []\n",
    "        # empty list to store average cruising speed values\n",
    "        avg_cruise_speed_values = []\n",
    "        for i in range(len(cruise_start)):\n",
    "            # get the original dataset index for start and end of cruise phase\n",
    "            start_index = cruise_start.loc[i,['index']].values[0]\n",
    "            end_index = cruise_end.loc[i,['index']].values[0]\n",
    "            # saves cruising phase with noise as a list of pandas series\n",
    "            self.cruise_with_vn.append(self.df.loc[start_index:end_index,'speed_mps'])\n",
    "            # calculate average cruise speed over the cruising phase\n",
    "            avg_cruise_speed = self.cruise_with_vn[i].mean()\n",
    "            # append the values\n",
    "            avg_cruise_speed_values.append(avg_cruise_speed)\n",
    "        # generates and saves the average crusing speed histogram object\n",
    "        self.avg_cs_obj = Attribute(np.array(avg_cruise_speed_values))\n",
    "\n",
    "        return self.avg_cs_obj\n",
    "\n",
    "    def extract_idle_duration(self):\n",
    "        \"\"\" Extracts idle duration data and generates a histogram object to store the data\n",
    "        \"\"\"\n",
    "        # get the timestamp for points = 4 (start of idle phase)\n",
    "        idle_start = self.points[self.points['points']==4]\n",
    "        idle_start = idle_start.loc[:,['timestamp']]\n",
    "        # the last labelled '4' is the end of a pulse with no subsequent pulse, therefore cannot be used to calculate idle\n",
    "        idle_start = idle_start.iloc[:-1,:]\n",
    "        # reset the index\n",
    "        idle_start.reset_index(drop = True, inplace=True)\n",
    "        # get the timestamp for points = 1 (end of idle phase)\n",
    "        idle_end = self.points[self.points['points']==1]\n",
    "        idle_end = idle_end.loc[:,['timestamp']]\n",
    "        # the first labelled '1' is the start of a pulse with no prior pulse, therefore cannot be used to calculate idle\n",
    "        idle_end = idle_end.iloc[1:,:]\n",
    "        # reset the index\n",
    "        idle_end.reset_index(drop = True, inplace=True)\n",
    "\n",
    "        # empty list to store cruising duration values\n",
    "        idle_duration_values = []\n",
    "        for i in range(len(idle_start)):\n",
    "            # calculate time difference\n",
    "            idle_duration = (idle_end.loc[i,'timestamp'] - idle_start.loc[i,'timestamp']).seconds\n",
    "            # append the values\n",
    "            idle_duration_values.append(idle_duration)\n",
    "        # generates and saves the idle duration histogram object\n",
    "        self.idle_obj = Attribute(np.array(idle_duration_values))\n",
    "\n",
    "        return self.idle_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/koeboonshyang/Documents/GitHub/MEng-V2I\n",
      "[1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.\n",
      " 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "# file_name = 'device12_oct_classifed_Day11to20.csv'\n",
    "# subdir = 'DC_generator'\n",
    "file_name = 'device12_oct_classified.csv'\n",
    "subdir = ''\n",
    "extract_obj = Extract_Hist(file_name, subdir)\n",
    "\n",
    "# extract_obj.extract_avg_cruise_speed()\n",
    "# accel_obj = extract_obj.extract_acceleration()\n",
    "# # decel_array = extract_obj.extract_deceleration()\n",
    "# print(extract_obj.points)\n",
    "# print(extract_obj.complete_labels)\n",
    "# # print(extract_obj.complete_labels)\n",
    "\n",
    "# #load initial gaussian parameters\n",
    "# param_obj = Gaussian_param()\n",
    "# #create a probability function object for acceleration with its acceleration histogram data\n",
    "# accel_prob_obj = Probability_Functions(accel_obj.bins, accel_obj.data_points,2)\n",
    "# #fit the histogram\n",
    "# hey = accel_prob_obj.NLLSR(param_obj)\n",
    "\n",
    "# #plot the histogram, each gaussian component and the combined gaussian curve\n",
    "# plt.plot(accel_prob_obj.x,accel_prob_obj.original_y,'b')\n",
    "# yy = accel_prob_obj.eqn_model(hey.params)\n",
    "# gaus_1 = accel_prob_obj.single_component(hey.params['alpha_1'],hey.params['sigma_1'],hey.params['meu_1'])\n",
    "# gaus_2 = accel_prob_obj.single_component(hey.params['alpha_2'],hey.params['sigma_2'],hey.params['meu_2'])\n",
    "# plt.plot(accel_prob_obj.x, yy,'r', label = 'best_fit')\n",
    "# plt.plot(accel_prob_obj.x, gaus_1,'g', label = 'gaus_1')\n",
    "# plt.plot(accel_prob_obj.x, gaus_2,'y', label = 'gaus_2')\n",
    "# plt.legend(loc='best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian_param():\n",
    "    \"\"\" Inital parameters for each Gaussian characteristic paramter \n",
    "    \"\"\"\n",
    "    LMparams = Parameters()\n",
    "    # there are two sets of values because the paper used 2 curve (n=2) to fit the distributions\n",
    "    LMparams.add('alpha_1', value = 1., min = 0)\n",
    "    LMparams.add('alpha_2', value = 1., min = 0)\n",
    "    LMparams.add('sigma_1', value = 1., min = 0)\n",
    "    LMparams.add('sigma_2', value = 1., min = 0)\n",
    "    LMparams.add('meu_1', value = 1., min = 0)\n",
    "    LMparams.add('meu_2', value = 1., min = 0)\n",
    "\n",
    "    return LMparams\n",
    "\n",
    "def Gaussian_idle_param():\n",
    "    \"\"\" Inital parameters for each Gaussian characteristic paramter \n",
    "    \"\"\"\n",
    "    LMparams = Parameters()\n",
    "    # there are two sets of values because the paper used 2 curve (n=2) to fit the distributions\n",
    "    LMparams.add('alpha_1', value = 100., min = 0)\n",
    "    LMparams.add('alpha_2', value = 50., min = 0)\n",
    "    LMparams.add('sigma_1', value = 100., min = 0)\n",
    "    LMparams.add('sigma_2', value = 100., min = 0)\n",
    "    LMparams.add('meu_1', value = 2500., min = 0)\n",
    "    LMparams.add('meu_2', value = 5000., min = 0)\n",
    "\n",
    "    return LMparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Probability_Functions(object):\n",
    "    def __init__(self, x, y, n):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.original_y = y\n",
    "        # the number of Gaussian distribution used to describe the data\n",
    "        self.n = n\n",
    "\n",
    "    def single_component(self, alpha_i, sigma_i, meu_i):\n",
    "        \"\"\" Returns the single Gaussian component as described in the sum of eqn (10)\n",
    "        \"\"\"\n",
    "        exp_component = -((self.x - meu_i)**2)/(2*(sigma_i**2))\n",
    "        fcn = (alpha_i/(sigma_i*math.sqrt(2*math.pi)))*np.exp(exp_component)\n",
    "        return fcn\n",
    "\n",
    "    def eqn_model(self, params):\n",
    "        \"\"\" Returns the Gaussian component sum as described in eqn (10)\n",
    "        \"\"\"\n",
    "        # runs a loop for the number of of Gaussian distibutions used to describe the data, and sums the single components \n",
    "        for component in range(1,self.n+1):\n",
    "            alpha_i = 'alpha_'+str(component)\n",
    "            sigma_i = 'sigma_'+str(component)\n",
    "            meu_i = 'meu_'+str(component)\n",
    "            if component == 1:\n",
    "                model = self.single_component(params[alpha_i],params[sigma_i],params[meu_i])\n",
    "            else:\n",
    "                model += self.single_component(params[alpha_i],params[sigma_i],params[meu_i])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fnc2min(self, params):\n",
    "        \"\"\" Returns the residuals for eqn_model\n",
    "        \"\"\"\n",
    "        return (self.y - self.eqn_model(params))\n",
    "\n",
    "    def NLLSR(self, LMparams):\n",
    "        \"\"\" Returns the result of the NLLSR using LMFit\n",
    "        \"\"\"\n",
    "        # uses least swuares method to minimize the parameters given by LMparams according to the residuals given by self.fnc2min\n",
    "        LMFitmin = Minimizer(self.fnc2min, LMparams)\n",
    "        LMFitResult = LMFitmin.minimize(method='least_squares')\n",
    "        lmfit.printfuncs.report_fit(LMFitResult.params)\n",
    "        self.params = LMFitResult.params\n",
    "\n",
    "        return LMFitResult\n",
    "\n",
    "    def single_cdf_component(self, x, alpha_i, sigma_i, meu_i):\n",
    "        \"\"\" Returns the single cdf component as described in the sum of eqn (12)\n",
    "        \"\"\"\n",
    "        erf_param = (x - meu_i)/(sigma_i * math.sqrt(2))\n",
    "        answer =  0.5 * alpha_i * (1 + special.erf(erf_param))\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def cdf(self, x, params):\n",
    "        \"\"\" Returns the cdf component sum as described in eqn (12)\n",
    "        \"\"\"\n",
    "        for component in range(1,self.n+1):\n",
    "            alpha_i = 'alpha_'+str(component)\n",
    "            sigma_i = 'sigma_'+str(component)\n",
    "            meu_i ='meu_'+str(component)\n",
    "\n",
    "            if component == 1:\n",
    "                answer =  self.single_cdf_component(x, params[alpha_i], params[sigma_i], params[meu_i])\n",
    "            else:\n",
    "                answer +=  self.single_cdf_component(x, params[alpha_i], params[sigma_i], params[meu_i])\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def normalised_cdf(self,x, LMparams):\n",
    "        \"\"\" Returns a normalised cdf as described in eqn (13)\n",
    "        \"\"\"\n",
    "        lim_inf_cdf = self.cdf(math.inf, LMparams)\n",
    "        answer = self.cdf(x, LMparams)/lim_inf_cdf\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def normalised_single_cdf(self,x, alpha_i, sigma_i, meu_i):\n",
    "        \"\"\" normalised cdf for a SINGLE component \n",
    "        \"\"\"\n",
    "        lim_inf_cdf = self.single_cdf_component(math.inf, alpha_i, sigma_i, meu_i)\n",
    "        answer = self.single_cdf_component(x, alpha_i, sigma_i, meu_i)/lim_inf_cdf\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def single_quantile_component(self, p, alpha_i, sigma_i, meu_i):\n",
    "        \"\"\" single inverse cdf component\n",
    "        \"\"\"\n",
    "        inv_erf_param = (2*p/alpha_i) - 1\n",
    "        answer =  meu_i + (sigma_i * math.sqrt(2) * special.erfinv(inv_erf_param))\n",
    "\n",
    "        return answer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class inv_cdf(object):\n",
    "    def __init__(self, prob_obj):\n",
    "        # create a numpy array ranging from 0 to 1 with granularity of e-3\n",
    "        # the array represents the possible p values that could be input to determine the attribute value, therefore\n",
    "        # granularity of the random number generator needs to be set to e-3\n",
    "        self.x = np.arange(0,1.001,0.001)\n",
    "        # genenrate an empty array with same shape as self.x\n",
    "        self.y = np.zeros(self.x.shape)\n",
    "        # save a local version of the probability object\n",
    "        self.prob_obj = prob_obj\n",
    "        # generate the lookup table for the ranges of x\n",
    "        self.fit()\n",
    "\n",
    "    def diff(self, x, a):\n",
    "        \"\"\" residual(?) for the fit method\n",
    "        \"\"\"\n",
    "        yt = self.prob_obj.normalised_cdf(x, self.prob_obj.params)\n",
    "        return (yt - a )**2\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\" Fits the inverse normalised cdf and generates a lookup table of sort\n",
    "        \"\"\"\n",
    "        # the for loop generates a lookup table correlating x and y based on the normalised cdf\n",
    "        for idx,x_value in enumerate(self.x):\n",
    "            res = sp_minimize(self.diff, 1.0, args=(x_value), method='Nelder-Mead', tol=1e-6)\n",
    "            self.y[idx] = res.x[0]\n",
    "\n",
    "    def get_value(self, p):\n",
    "        \"\"\" Gives corresponding attribute value depending on p (ranging from 0 to 1)\n",
    "        \"\"\"\n",
    "        x_copy = np.copy(self.x)\n",
    "        # find the index at which the x is equal to the input p\n",
    "        index = np.where(x_copy == p)\n",
    "        # gets the corresponding attribute value depending on the index\n",
    "        value = self.y[index]\n",
    "\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create object storing histogram datapoints\n",
    "accel_obj = Acceleration()\n",
    "print(accel_obj.bins)\n",
    "#load initial gaussian parameters\n",
    "param_obj = Gaussian_param()\n",
    "#create a probability function object for acceleration with its acceleration histogram data\n",
    "accel_prob_obj = Probability_Functions(accel_obj.bins, accel_obj.data_points,2)\n",
    "#fit the histogram\n",
    "hey = accel_prob_obj.NLLSR(param_obj)\n",
    "\n",
    "#plot the histogram, each gaussian component and the combined gaussian curve\n",
    "plt.plot(accel_prob_obj.x,accel_prob_obj.original_y,'b')\n",
    "yy = accel_prob_obj.eqn_model(hey.params)\n",
    "gaus_1 = accel_prob_obj.single_component(hey.params['alpha_1'],hey.params['sigma_1'],hey.params['meu_1'])\n",
    "gaus_2 = accel_prob_obj.single_component(hey.params['alpha_2'],hey.params['sigma_2'],hey.params['meu_2'])\n",
    "plt.plot(accel_prob_obj.x, yy,'r', label = 'best_fit')\n",
    "plt.plot(accel_prob_obj.x, gaus_1,'g', label = 'gaus_1')\n",
    "plt.plot(accel_prob_obj.x, gaus_2,'y', label = 'gaus_2')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "print(accel_prob_obj.cdf(0.8, hey.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0.33\n",
    "erf_param = (x - 0.83011822)/(0.21988047 * math.sqrt(2))\n",
    "print(erf_param)\n",
    "answer =  12.5916156 * (1 + special.erf(erf_param))\n",
    "print(answer)\n",
    "\n",
    "x = math.inf\n",
    "erf_param = (x - 0.83011822)/(0.21988047 * math.sqrt(2))\n",
    "print(erf_param)\n",
    "lim =  12.5916156 * (1 + special.erf(erf_param))\n",
    "print(answer/lim)\n",
    "\n",
    "\n",
    "x = np.linspace(0,2,1000)\n",
    "#plot the cdf of the sum of gaussian\n",
    "lim_inf_cdf = accel_prob_obj.cdf(math.inf, hey.params)\n",
    "yy = accel_prob_obj.normalised_cdf(x,hey.params)\n",
    "cdf_1 = accel_prob_obj.single_cdf_component(x,hey.params['alpha_1'],hey.params['sigma_1'],hey.params['meu_1'])/lim_inf_cdf\n",
    "cdf_2 = accel_prob_obj.single_cdf_component(x,hey.params['alpha_2'],hey.params['sigma_2'],hey.params['meu_2'])/lim_inf_cdf\n",
    "plt.plot(yy,x,'r', label = 'best_fit')\n",
    "plt.plot(cdf_1,x,'g', label = 'cdf_1')\n",
    "plt.plot(cdf_2,x,'y', label = 'cdf_2')\n",
    "plt.legend(loc='best')\n",
    "# plt.show()\n",
    "\n",
    "inv_cdf_obj = inv_cdf(accel_prob_obj)\n",
    "print(inv_cdf_obj.get_value(0))\n",
    "\n",
    "print(accel_prob_obj.normalised_cdf(0.718,hey.params))\n",
    "\n",
    "# q_1 = accel_prob_obj.single_quantile_component(x*lim_inf_cdf,hey.params['alpha_1'],hey.params['sigma_1'],hey.params['meu_1'])\n",
    "# q_2 = accel_prob_obj.single_quantile_component(x*lim_inf_cdf,hey.params['alpha_2'],hey.params['sigma_2'],hey.params['meu_2'])\n",
    "\n",
    "plt.plot(inv_cdf_obj.x,inv_cdf_obj.y,  label = 'minimized')\n",
    "# plt.plot(x,q_1,label = 'idk_1')\n",
    "# plt.plot(x,q_2,label = 'idk_2')\n",
    "plt.title(r'$f^{-1}(x)$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='best')\n",
    "# # plt.savefig(\"function_inverse.png\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create object storing histogram datapoints\n",
    "cd_obj = Cruising_Duration()\n",
    "print(cd_obj.bins)\n",
    "#load initial gaussian parameters\n",
    "param_obj = Gaussian_param()\n",
    "#create a probability function object for acceleration with its acceleration histogram data\n",
    "cd_prob_obj = Probability_Functions(cd_obj.bins, cd_obj.data_points,2)\n",
    "#fit the histogram\n",
    "hey = cd_prob_obj.NLLSR(param_obj)\n",
    "\n",
    "#plot the histogram, each gaussian component and the combined gaussian curve\n",
    "plt.plot(cd_prob_obj.x,cd_prob_obj.original_y,'b')\n",
    "yy = cd_prob_obj.eqn_model(hey.params)\n",
    "gaus_1 = cd_prob_obj.single_component(hey.params['alpha_1'],hey.params['sigma_1'],hey.params['meu_1'])\n",
    "gaus_2 = cd_prob_obj.single_component(hey.params['alpha_2'],hey.params['sigma_2'],hey.params['meu_2'])\n",
    "plt.plot(cd_prob_obj.x, yy,'r', label = 'best_fit')\n",
    "plt.plot(cd_prob_obj.x, gaus_1,'g', label = 'gaus_1')\n",
    "plt.plot(cd_prob_obj.x, gaus_2,'y', label = 'gaus_2')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create object storing histogram datapoints\n",
    "avg_cs_obj = Average_Crusing_Speed()\n",
    "print(avg_cs_obj.bins)\n",
    "#load initial gaussian parameters\n",
    "param_obj = Gaussian_param()\n",
    "#create a probability function object for acceleration with its acceleration histogram data\n",
    "avg_cs_prob_obj = Probability_Functions(avg_cs_obj.bins, avg_cs_obj.data_points,2)\n",
    "#fit the histogram\n",
    "hey = avg_cs_prob_obj.NLLSR(param_obj)\n",
    "\n",
    "#plot the histogram, each gaussian component and the combined gaussian curve\n",
    "plt.plot(avg_cs_prob_obj.x,avg_cs_prob_obj.original_y,'b')\n",
    "yy = avg_cs_prob_obj.eqn_model(hey.params)\n",
    "gaus_1 = avg_cs_prob_obj.single_component(hey.params['alpha_1'],hey.params['sigma_1'],hey.params['meu_1'])\n",
    "gaus_2 = avg_cs_prob_obj.single_component(hey.params['alpha_2'],hey.params['sigma_2'],hey.params['meu_2'])\n",
    "plt.plot(avg_cs_prob_obj.x, yy,'r', label = 'best_fit')\n",
    "plt.plot(avg_cs_prob_obj.x, gaus_1,'g', label = 'gaus_1')\n",
    "plt.plot(avg_cs_prob_obj.x, gaus_2,'y', label = 'gaus_2')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create object storing histogram datapoints\n",
    "decel_obj = Deceleration()\n",
    "print(decel_obj.bins)\n",
    "#load initial gaussian parameters\n",
    "param_obj = Gaussian_param()\n",
    "#create a probability function object for deceleration with its deceleration histogram data\n",
    "decel_prob_obj = Probability_Functions(decel_obj.bins, decel_obj.data_points,2)\n",
    "#fit the histogram\n",
    "hey = decel_prob_obj.NLLSR(param_obj)\n",
    "\n",
    "#plot the histogram, each gaussian component and the combined gaussian curve\n",
    "plt.plot(decel_prob_obj.x,decel_prob_obj.original_y,'b')\n",
    "yy = decel_prob_obj.eqn_model(hey.params)\n",
    "gaus_1 = decel_prob_obj.single_component(hey.params['alpha_1'],hey.params['sigma_1'],hey.params['meu_1'])\n",
    "gaus_2 = decel_prob_obj.single_component(hey.params['alpha_2'],hey.params['sigma_2'],hey.params['meu_2'])\n",
    "plt.plot(decel_prob_obj.x, yy,'r', label = 'best_fit')\n",
    "plt.plot(decel_prob_obj.x, gaus_1,'g', label = 'gaus_1')\n",
    "plt.plot(decel_prob_obj.x, gaus_2,'y', label = 'gaus_2')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Velocity_Noise(object):\n",
    "    def __init__(self,t,y):\n",
    "        self.t = t\n",
    "        self.y = y\n",
    "        self.original_y = y\n",
    "        self.original_y_mean = self.original_y.mean()\n",
    "\n",
    "    def set_t(self, t):\n",
    "        \"\"\"Set t values\n",
    "        \"\"\"\n",
    "        self.t = t\n",
    "\n",
    "    def subtract_avg(self):\n",
    "        \"\"\"Removes the average speed from the observations\n",
    "        \"\"\"\n",
    "        self.y = self.y - self.original_y_mean\n",
    "        return self.y\n",
    "\n",
    "    def subtract(self, array):\n",
    "        self.y = self.y - array\n",
    "        return self.y\n",
    "\n",
    "    def single_component(self, A_i_FS, w_i_FS, phi_i_FS):\n",
    "        \"\"\" Returns a single velocity noise component as described in the sum of eqn (5)\n",
    "        \"\"\"\n",
    "        return A_i_FS * np.sin( (w_i_FS*self.t) + phi_i_FS )\n",
    "     \n",
    "    def eqn_model(self, params):\n",
    "        \"\"\" Returns the velocity noise FS model as described in eqn (5)\n",
    "        \"\"\"\n",
    "        # put all the paramters in a list\n",
    "        A_FS = [params['A1_FS'],params['A2_FS'],params['A3_FS']]\n",
    "        w_FS = [params['w1_FS'],params['w2_FS'],params['w3_FS']]\n",
    "        phi_FS = [params['phi1_FS'],params['phi2_FS'],params['phi3_FS']]\n",
    "\n",
    "        # equation (5), sum of 3 components\n",
    "        model = self.single_component(A_FS[0],w_FS[0], phi_FS[0])\n",
    "        model += self.single_component(A_FS[1],w_FS[1], phi_FS[1])\n",
    "        model += self.single_component(A_FS[2],w_FS[2], phi_FS[2])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def final_curve(self):\n",
    "        \"\"\" Returns the final fitted curve\n",
    "        \"\"\"\n",
    "        final = self.eqn_model(self.LF_params.params)\n",
    "        final += self.eqn_model(self.MF_params.params)\n",
    "        final += self.eqn_model(self.HF_params.params)\n",
    "\n",
    "        return final\n",
    "\n",
    "    def fit_all(self, LF_param = LF_Noise(), MF_param = MF_Noise(), HF_param = HF_Noise()):\n",
    "        \"\"\" Fits the velicoty noise for all FS components after self.subtract_avg() has been called\n",
    "        \"\"\"\n",
    "        # fits the velocity noise for LF components\n",
    "        self.LF_fit(LF_param)\n",
    "        # subtract the LF components from the velocity noise\n",
    "        self.subtract(self.eqn_model(self.LF_params.params))\n",
    "        # fits the velocity noise for MF components\n",
    "        self.MF_fit(MF_param)\n",
    "        # subtract the MF components from the velocity noise\n",
    "        self.subtract(self.eqn_model(self.MF_params.params))\n",
    "        # fits the velocity noise for HF components\n",
    "        self.HF_fit(HF_param)\n",
    "\n",
    "    def LF_fit(self, init_params = LF_Noise()):\n",
    "        \"\"\" Fits and returns the component parameters for LF noise\n",
    "        \"\"\"\n",
    "        self.LF_params = self.NLLSR(init_params)\n",
    "\n",
    "        return self.LF_params\n",
    "    \n",
    "    def MF_fit(self, init_params = MF_Noise()):\n",
    "        \"\"\" Fits and returns the component parameters for HF noise\n",
    "        \"\"\"\n",
    "        self.MF_params = self.NLLSR(init_params)\n",
    "\n",
    "        return self.MF_params\n",
    "\n",
    "    def HF_fit(self, init_params = HF_Noise()):\n",
    "        \"\"\" Fits and returns the component parameters for HF noise\n",
    "        \"\"\"\n",
    "        self.HF_params = self.NLLSR(init_params)\n",
    "\n",
    "        return self.HF_params\n",
    "\n",
    "    def fnc2min(self, params):\n",
    "        \"\"\" Returns the residuals (eqn 7) for the model for when running all 3 components at once\n",
    "        \"\"\"\n",
    "        return (self.y - self.eqn_model(params))\n",
    "\n",
    "    # def fnc2min(self, params):\n",
    "    #     \"\"\" Returns the residuals (eqn 7) for the model for when running one component at a time\n",
    "    #     \"\"\"\n",
    "    #     return (self.y - self.single_component(params['A_FS'], params['w_FS'], params['phi_FS']))\n",
    "\n",
    "\n",
    "    def NLLSR(self, LMparams):\n",
    "        \"\"\" Returns the result of the NLLSR using LMFit\n",
    "        \"\"\"\n",
    "        # uses least swuares method to minimize the parameters given by LMparams according to the residuals given by self.fnc2min\n",
    "        LMFitmin = Minimizer(self.fnc2min, LMparams)\n",
    "        LMFitResult = LMFitmin.minimize(method='least_squares')\n",
    "        lmfit.printfuncs.report_fit(LMFitResult.params)\n",
    "\n",
    "        return LMFitResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/koeboonshyang/Documents/GitHub/MEng-V2I\n",
      "[[Variables]]\n",
      "    A1_FS:    134.386615 +/- 5088017.58 (3786104.42%) (init = 10)\n",
      "    A2_FS:    16.2435771 +/- 19354.5807 (119152.21%) (init = 10)\n",
      "    A3_FS:   -13.3175383 +/- 19396.3730 (145645.33%) (init = 10)\n",
      "    w1_FS:    3.3974e-08 +/- 0.00313057 (9214518.42%) (init = 0)\n",
      "    w2_FS:    0.05782469 +/- 3.38597446 (5855.59%) (init = 0.03141593)\n",
      "    w3_FS:    0.06283185 +/- 3.28310183 (5225.22%) (init = 0.06283185)\n",
      "    phi1_FS: -0.01770171 +/- 670.368619 (3787026.67%) (init = 0)\n",
      "    phi2_FS: -0.49549144 +/- 95.6425907 (19302.57%) (init = 0)\n",
      "    phi3_FS: -0.63828219 +/- 94.4439242 (14796.58%) (init = 0)\n",
      "[[Correlations]] (unreported correlations are < 0.100)\n",
      "    C(A1_FS, phi1_FS)   =  1.000\n",
      "    C(A2_FS, A3_FS)     = -1.000\n",
      "    C(w3_FS, phi3_FS)   = -1.000\n",
      "    C(w2_FS, phi2_FS)   = -1.000\n",
      "    C(A2_FS, w3_FS)     = -1.000\n",
      "    C(A3_FS, w3_FS)     =  1.000\n",
      "    C(A3_FS, w2_FS)     = -1.000\n",
      "    C(A2_FS, w2_FS)     =  1.000\n",
      "    C(A2_FS, phi3_FS)   =  1.000\n",
      "    C(A3_FS, phi3_FS)   = -1.000\n",
      "    C(A3_FS, phi2_FS)   =  1.000\n",
      "    C(A2_FS, phi2_FS)   = -1.000\n",
      "    C(w2_FS, w3_FS)     = -1.000\n",
      "    C(w2_FS, phi3_FS)   =  1.000\n",
      "    C(w3_FS, phi2_FS)   =  1.000\n",
      "    C(phi2_FS, phi3_FS) = -1.000\n",
      "    C(A3_FS, w1_FS)     = -0.940\n",
      "    C(A2_FS, w1_FS)     =  0.940\n",
      "    C(w1_FS, w2_FS)     =  0.939\n",
      "    C(w1_FS, phi2_FS)   = -0.939\n",
      "    C(w1_FS, phi3_FS)   =  0.939\n",
      "    C(w1_FS, w3_FS)     = -0.939\n",
      "    C(phi1_FS, phi3_FS) =  0.555\n",
      "    C(phi1_FS, phi2_FS) = -0.555\n",
      "    C(A1_FS, phi3_FS)   =  0.555\n",
      "    C(A1_FS, w3_FS)     = -0.555\n",
      "    C(A1_FS, A2_FS)     =  0.555\n",
      "    C(A1_FS, A3_FS)     = -0.555\n",
      "    C(A1_FS, w2_FS)     =  0.555\n",
      "    C(A1_FS, phi2_FS)   = -0.555\n",
      "    C(w3_FS, phi1_FS)   = -0.554\n",
      "    C(A2_FS, phi1_FS)   =  0.554\n",
      "    C(A3_FS, phi1_FS)   = -0.554\n",
      "    C(w2_FS, phi1_FS)   =  0.554\n",
      "    C(A1_FS, w1_FS)     =  0.237\n",
      "    C(w1_FS, phi1_FS)   =  0.236\n",
      "[[Variables]]\n",
      "    A1_FS:   -0.11633890 +/- 0.00209233 (1.80%) (init = 10)\n",
      "    A2_FS:    9.71192458 +/- 31908.3222 (328547.88%) (init = 10)\n",
      "    A3_FS:   -9.83588989 +/- 31908.3213 (324407.06%) (init = 10)\n",
      "    w1_FS:    1.14915074 +/- 9.4721e-04 (0.08%) (init = 0.1256637)\n",
      "    w2_FS:    0.16749805 +/- 0.93282515 (556.92%) (init = 0.1884956)\n",
      "    w3_FS:    0.16806616 +/- 0.92195858 (548.57%) (init = 0.1884956)\n",
      "    phi1_FS:  1.11034772 +/- 0.03677079 (3.31%) (init = 0)\n",
      "    phi2_FS:  0.56083360 +/- 33.0180689 (5887.32%) (init = 0)\n",
      "    phi3_FS:  0.54072383 +/- 32.6365700 (6035.72%) (init = 0)\n",
      "[[Correlations]] (unreported correlations are < 0.100)\n",
      "    C(A2_FS, A3_FS)     = -1.000\n",
      "    C(w3_FS, phi3_FS)   = -1.000\n",
      "    C(w2_FS, phi2_FS)   = -1.000\n",
      "    C(A2_FS, w3_FS)     = -1.000\n",
      "    C(A3_FS, w3_FS)     =  1.000\n",
      "    C(A3_FS, w2_FS)     = -1.000\n",
      "    C(A2_FS, w2_FS)     =  1.000\n",
      "    C(A2_FS, phi3_FS)   =  1.000\n",
      "    C(A3_FS, phi3_FS)   = -1.000\n",
      "    C(A3_FS, phi2_FS)   =  1.000\n",
      "    C(A2_FS, phi2_FS)   = -1.000\n",
      "    C(w2_FS, phi3_FS)   =  1.000\n",
      "    C(w3_FS, phi2_FS)   =  1.000\n",
      "    C(w2_FS, w3_FS)     = -1.000\n",
      "    C(phi2_FS, phi3_FS) = -1.000\n",
      "    C(w1_FS, phi1_FS)   = -0.873\n",
      "    C(phi1_FS, phi2_FS) =  0.158\n",
      "    C(w2_FS, phi1_FS)   = -0.158\n",
      "    C(A3_FS, phi1_FS)   =  0.158\n",
      "    C(A2_FS, phi1_FS)   = -0.158\n",
      "    C(w3_FS, phi1_FS)   =  0.158\n",
      "    C(phi1_FS, phi3_FS) = -0.158\n",
      "    C(w1_FS, phi2_FS)   = -0.136\n",
      "    C(w1_FS, w2_FS)     =  0.136\n",
      "    C(A3_FS, w1_FS)     = -0.136\n",
      "    C(A2_FS, w1_FS)     =  0.136\n",
      "    C(w1_FS, w3_FS)     = -0.136\n",
      "    C(w1_FS, phi3_FS)   =  0.136\n",
      "[[Variables]]\n",
      "    A1_FS:   -0.03523835 +/- 0.00207174 (5.88%) (init = 1)\n",
      "    A2_FS:   -0.03533814 +/- 0.00207533 (5.87%) (init = 1)\n",
      "    A3_FS:   -0.02134465 +/- 0.00206862 (9.69%) (init = 1)\n",
      "    w1_FS:    1.57079633 +/- 0.00309056 (0.20%) (init = 1.570796)\n",
      "    w2_FS:    2.20971221 +/- 0.00306329 (0.14%) (init = 2.356194)\n",
      "    w3_FS:    3.09559683 +/- 0.00508280 (0.16%) (init = 3.141593)\n",
      "    phi1_FS:  0.19997976 +/- 0.11993743 (59.97%) (init = 0)\n",
      "    phi2_FS: -0.13296904 +/- 0.11928171 (89.71%) (init = 0)\n",
      "    phi3_FS:  3.14159265 +/- 0.19818510 (6.31%) (init = 0)\n",
      "[[Correlations]] (unreported correlations are < 0.100)\n",
      "    C(w3_FS, phi3_FS) = -0.872\n",
      "    C(w1_FS, phi1_FS) = -0.872\n",
      "    C(w2_FS, phi2_FS) = -0.872\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # loads the csv file and extract the attribute informations\n",
    "    file_name = 'device12_oct_classified.csv'\n",
    "    subdir = ''\n",
    "    extract_obj = Extract_Hist(file_name, subdir)\n",
    "    # get the slice of ONLY cruising period\n",
    "    cruising_data = extract_obj.cruise_with_vn[270]\n",
    "    # create a numpy array of just t values starting at t=1\n",
    "    t = np.linspace(1,len(cruising_data),len(cruising_data))\n",
    "    # create a numpy array of speed_mps values\n",
    "    y = cruising_data.to_numpy()\n",
    "    # interpolate linearly and make timesteps finer (0.001s)\n",
    "    from scipy.interpolate import interp1d\n",
    "    f = interp1d(t, y)\n",
    "    t = np.linspace(1,len(cruising_data),1000*len(cruising_data))\n",
    "    y = f(t)\n",
    "\n",
    "    # initialise the VN object\n",
    "    vn_obj = Velocity_Noise(t,y)\n",
    "    # deduct the average from the cruising period speed values (from fig3a to fig3b) and store as y\n",
    "    y = vn_obj.subtract_avg()\n",
    "\n",
    "    original_y = y\n",
    "    \n",
    "    # perform NLLSR with the initial parameters suggested by LMParams\n",
    "    hi = vn_obj.fit_all()\n",
    "    yy = vn_obj.final_curve()\n",
    "    \n",
    "    # pulse_duration=500\n",
    "    # driving_pulse = DP(pulse_duration, extract_obj)\n",
    "\n",
    "    # plt.plot(t,y,'b')\n",
    "    # plt.plot(t, yy,'r', label = 'best_fit')\n",
    "    # plt.legend(loc='best')\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    # driving_pulse.generate_drive_cycle(vn_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[Variables]]\n",
      "    alpha_1:  255.231550 (init = 1)\n",
      "    alpha_2:  7.83294721 (init = 1)\n",
      "    sigma_1:  0.45196004 (init = 1)\n",
      "    sigma_2:  0.03448800 (init = 1)\n",
      "    meu_1:    0.67201241 (init = 1)\n",
      "    meu_2:    4.13706078 (init = 1)\n",
      "[[Variables]]\n",
      "    alpha_1:  22804.7981 +/- 1263.10703 (5.54%) (init = 1)\n",
      "    alpha_2:  22654.6406 +/- 1059.76683 (4.68%) (init = 1)\n",
      "    sigma_1:  96.0837680 +/- 4.75402482 (4.95%) (init = 1)\n",
      "    sigma_2:  36.6299523 +/- 0.93307896 (2.55%) (init = 1)\n",
      "    meu_1:    179.185018 +/- 6.25218236 (3.49%) (init = 1)\n",
      "    meu_2:    49.7778553 +/- 0.57990508 (1.16%) (init = 1)\n",
      "[[Correlations]] (unreported correlations are < 0.100)\n",
      "    C(alpha_2, meu_1)   =  0.936\n",
      "    C(alpha_1, alpha_2) = -0.936\n",
      "    C(alpha_1, sigma_1) =  0.913\n",
      "    C(alpha_2, sigma_2) =  0.885\n",
      "    C(alpha_1, meu_1)   = -0.879\n",
      "    C(alpha_2, sigma_1) = -0.875\n",
      "    C(sigma_1, meu_1)   = -0.818\n",
      "    C(alpha_1, sigma_2) = -0.809\n",
      "    C(sigma_2, meu_1)   =  0.809\n",
      "    C(sigma_1, sigma_2) = -0.718\n",
      "    C(meu_1, meu_2)     =  0.349\n",
      "    C(alpha_2, meu_2)   =  0.321\n",
      "    C(alpha_1, meu_2)   = -0.316\n",
      "    C(sigma_2, meu_2)   =  0.308\n",
      "    C(sigma_1, meu_2)   = -0.162\n",
      "[[Variables]]\n",
      "    alpha_1:  495.429763 +/- 11.1916295 (2.26%) (init = 1)\n",
      "    alpha_2:  186.800520 +/- 10.4772532 (5.61%) (init = 1)\n",
      "    sigma_1:  1.92014702 +/- 0.03932747 (2.05%) (init = 1)\n",
      "    sigma_2:  1.07864944 +/- 0.03041939 (2.82%) (init = 1)\n",
      "    meu_1:    7.00008899 +/- 0.04982277 (0.71%) (init = 1)\n",
      "    meu_2:    3.64726140 +/- 0.03409179 (0.93%) (init = 1)\n",
      "[[Correlations]] (unreported correlations are < 0.100)\n",
      "    C(alpha_1, alpha_2) = -0.946\n",
      "    C(alpha_2, meu_1)   =  0.943\n",
      "    C(alpha_1, sigma_1) =  0.932\n",
      "    C(alpha_1, meu_1)   = -0.903\n",
      "    C(alpha_2, sigma_1) = -0.902\n",
      "    C(alpha_2, sigma_2) =  0.857\n",
      "    C(sigma_1, meu_1)   = -0.856\n",
      "    C(meu_1, meu_2)     =  0.840\n",
      "    C(alpha_1, meu_2)   = -0.826\n",
      "    C(alpha_2, meu_2)   =  0.823\n",
      "    C(sigma_2, meu_1)   =  0.790\n",
      "    C(alpha_1, sigma_2) = -0.785\n",
      "    C(sigma_1, meu_2)   = -0.739\n",
      "    C(sigma_2, meu_2)   =  0.706\n",
      "    C(sigma_1, sigma_2) = -0.682\n",
      "[[Variables]]\n",
      "    alpha_1:  141.708151 +/- 1.23273831 (0.87%) (init = 1)\n",
      "    alpha_2:  6.50350679 +/- 0.53017930 (8.15%) (init = 1)\n",
      "    sigma_1:  0.32365849 +/- 0.00294162 (0.91%) (init = 1)\n",
      "    sigma_2:  0.04353422 +/- 0.00333334 (7.66%) (init = 1)\n",
      "    meu_1:    0.72665533 +/- 0.00337882 (0.46%) (init = 1)\n",
      "    meu_2:    0.39277041 +/- 0.00295832 (0.75%) (init = 1)\n",
      "[[Correlations]] (unreported correlations are < 0.100)\n",
      "    C(alpha_2, sigma_2) =  0.696\n",
      "    C(alpha_1, sigma_1) =  0.645\n",
      "    C(alpha_2, meu_1)   =  0.590\n",
      "    C(alpha_1, alpha_2) = -0.544\n",
      "    C(sigma_2, meu_1)   =  0.400\n",
      "    C(alpha_1, sigma_2) = -0.372\n",
      "    C(alpha_2, sigma_1) = -0.357\n",
      "    C(alpha_1, meu_1)   = -0.321\n",
      "    C(sigma_1, sigma_2) = -0.236\n",
      "    C(sigma_1, meu_1)   = -0.211\n",
      "    C(sigma_1, meu_2)   =  0.115\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-8d4d02d81dfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpulse_duration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdriving_pulse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpulse_duration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'best_fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-290590de931d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pulse_duration, extract_obj)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_select_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters_for_drive_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccel_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecel_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcruising_duration_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_cruising_speed_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m_array_repr_implementation\u001b[0;34m(arr, max_line_width, precision, suppress_small, array2string)\u001b[0m\n\u001b[1;32m   1401\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m         lst = array2string(arr, max_line_width, precision, suppress_small,\n\u001b[0;32m-> 1403\u001b[0;31m                            ', ', prefix, suffix=suffix)\n\u001b[0m\u001b[1;32m   1404\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# show zero-length shape unless it is (0,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"[], shape=%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36marray2string\u001b[0;34m(a, max_line_width, precision, suppress_small, separator, prefix, style, formatter, threshold, edgeitems, sign, floatmode, suffix, **kwarg)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"[]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_array2string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0mrepr_running\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0mrepr_running\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m_array2string\u001b[0;34m(a, options, separator, prefix)\u001b[0m\n\u001b[1;32m    517\u001b[0m     lst = _formatArray(a, format_function, options['linewidth'],\n\u001b[1;32m    518\u001b[0m                        \u001b[0mnext_line_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edgeitems'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                        summary_insert, options['legacy'])\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m_formatArray\u001b[0;34m(a, format_function, line_width, next_line_prefix, separator, edge_items, summary_insert, legacy)\u001b[0m\n\u001b[1;32m    838\u001b[0m         return recurser(index=(),\n\u001b[1;32m    839\u001b[0m                         \u001b[0mhanging_indent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext_line_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m                         curr_width=line_width)\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;31m# recursive closures have a cyclic reference to themselves, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36mrecurser\u001b[0;34m(index, hanging_indent, curr_width)\u001b[0m\n\u001b[1;32m    794\u001b[0m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecurser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_hanging_indent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m                 s, line = _extendLine(\n\u001b[0;32m--> 796\u001b[0;31m                     s, line, word, elem_width, hanging_indent, legacy)\n\u001b[0m\u001b[1;32m    797\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m_extendLine\u001b[0;34m(s, line, word, line_width, next_line_prefix, legacy)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mneeds_wrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_line_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0mline\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pulse_duration=1000\n",
    "driving_pulse = DP(pulse_duration, extract_obj)\n",
    "\n",
    "plt.plot(t,y,'b')\n",
    "plt.plot(t, yy,'r', label = 'best_fit')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "driving_pulse.generate_drive_cycle(vn_obj)"
   ]
  },
  {
   "source": [
    "# the code below plots all the histograms for all the attributes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the histogram, each gaussian component and the combined gaussian curve\n",
    "plt.plot(extract_obj.accel_obj.bins,extract_obj.accel_obj.data_points,'b')\n",
    "\n",
    "#load initial gaussian parameters\n",
    "param_obj = Gaussian_param()\n",
    "#create a probability function object for attribute with its attribute histogram data\n",
    "attribute_prob_obj = Probability_Functions(extract_obj.accel_obj.bins, extract_obj.accel_obj.data_points,2)\n",
    "#fit the histogram\n",
    "fitted_obj = attribute_prob_obj.NLLSR(param_obj)\n",
    "\n",
    "yy = attribute_prob_obj.eqn_model(fitted_obj.params)\n",
    "gaus_1 = attribute_prob_obj.single_component(fitted_obj.params['alpha_1'],fitted_obj.params['sigma_1'],fitted_obj.params['meu_1'])\n",
    "gaus_2 = attribute_prob_obj.single_component(fitted_obj.params['alpha_2'],fitted_obj.params['sigma_2'],fitted_obj.params['meu_2'])\n",
    "plt.plot(attribute_prob_obj.x, yy,'r', label = 'best_fit')\n",
    "plt.plot(attribute_prob_obj.x, gaus_1,'g', label = 'gaus_1')\n",
    "plt.plot(attribute_prob_obj.x, gaus_2,'y', label = 'gaus_2')\n",
    "plt.xlabel('Acceleration (m/s^2)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(driving_pulse.accel_inv_cdf_obj.x,driving_pulse.accel_inv_cdf_obj.y,  label = 'minimized')\n",
    "plt.title(r'$f^{-1}(x)$')\n",
    "plt.xlabel('Uniformly distributed random number')\n",
    "plt.ylabel('Acceleration (m/s^2)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "#plot the histogram, each gaussian component and the combined gaussian curve\n",
    "plt.plot(extract_obj.decel_obj.bins,extract_obj.decel_obj.data_points,'b')\n",
    "\n",
    "#load initial gaussian parameters\n",
    "param_obj = Gaussian_param()\n",
    "#create a probability function object for attribute with its attribute histogram data\n",
    "attribute_prob_obj = Probability_Functions(extract_obj.decel_obj.bins, extract_obj.decel_obj.data_points,2)\n",
    "#fit the histogram\n",
    "fitted_obj = attribute_prob_obj.NLLSR(param_obj)\n",
    "\n",
    "yy = attribute_prob_obj.eqn_model(fitted_obj.params)\n",
    "gaus_1 = attribute_prob_obj.single_component(fitted_obj.params['alpha_1'],fitted_obj.params['sigma_1'],fitted_obj.params['meu_1'])\n",
    "gaus_2 = attribute_prob_obj.single_component(fitted_obj.params['alpha_2'],fitted_obj.params['sigma_2'],fitted_obj.params['meu_2'])\n",
    "plt.plot(attribute_prob_obj.x, yy,'r', label = 'best_fit')\n",
    "plt.plot(attribute_prob_obj.x, gaus_1,'g', label = 'gaus_1')\n",
    "plt.plot(attribute_prob_obj.x, gaus_2,'y', label = 'gaus_2')\n",
    "plt.xlabel('Deceleration (m/s^2)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(driving_pulse.decel_inv_cdf_obj.x,driving_pulse.decel_inv_cdf_obj.y,  label = 'minimized')\n",
    "plt.title(r'$f^{-1}(x)$')\n",
    "plt.xlabel('Uniformly distributed random number')\n",
    "plt.ylabel('Deceleration (m/s^2)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "#plot the histogram, each gaussian component and the combined gaussian curve\n",
    "plt.plot(extract_obj.cd_obj.bins,extract_obj.cd_obj.data_points,'b')\n",
    "\n",
    "#load initial gaussian parameters\n",
    "param_obj = Gaussian_param()\n",
    "#create a probability function object for attribute with its attribute histogram data\n",
    "attribute_prob_obj = Probability_Functions(extract_obj.cd_obj.bins, extract_obj.cd_obj.data_points,2)\n",
    "#fit the histogram\n",
    "fitted_obj = attribute_prob_obj.NLLSR(param_obj)\n",
    "# # create inverse cdf object\n",
    "# inv_cdf_obj = inv_cdf(attribute_prob_obj)\n",
    "\n",
    "yy = attribute_prob_obj.eqn_model(fitted_obj.params)\n",
    "gaus_1 = attribute_prob_obj.single_component(fitted_obj.params['alpha_1'],fitted_obj.params['sigma_1'],fitted_obj.params['meu_1'])\n",
    "gaus_2 = attribute_prob_obj.single_component(fitted_obj.params['alpha_2'],fitted_obj.params['sigma_2'],fitted_obj.params['meu_2'])\n",
    "plt.plot(attribute_prob_obj.x, yy,'r', label = 'best_fit')\n",
    "plt.plot(attribute_prob_obj.x, gaus_1,'g', label = 'gaus_1')\n",
    "plt.plot(attribute_prob_obj.x, gaus_2,'y', label = 'gaus_2')\n",
    "plt.xlabel('Cruise Duration (s)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(driving_pulse.cruising_duration_inv_cdf_obj.x,driving_pulse.cruising_duration_inv_cdf_obj.y,  label = 'minimized')\n",
    "plt.title(r'$f^{-1}(x)$')\n",
    "plt.xlabel('Uniformly distributed random number')\n",
    "plt.ylabel('Cruise Duration (s)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "#plot the histogram, each gaussian component and the combined gaussian curve\n",
    "plt.plot(extract_obj.avg_cs_obj.bins,extract_obj.avg_cs_obj.data_points,'b')\n",
    "\n",
    "#load initial gaussian parameters\n",
    "param_obj = Gaussian_param()\n",
    "#create a probability function object for attribute with its attribute histogram data\n",
    "attribute_prob_obj = Probability_Functions(extract_obj.avg_cs_obj.bins, extract_obj.avg_cs_obj.data_points,2)\n",
    "#fit the histogram\n",
    "fitted_obj = attribute_prob_obj.NLLSR(param_obj)\n",
    "# # create inverse cdf object\n",
    "# inv_cdf_obj = inv_cdf(attribute_prob_obj)\n",
    "\n",
    "yy = attribute_prob_obj.eqn_model(fitted_obj.params)\n",
    "gaus_1 = attribute_prob_obj.single_component(fitted_obj.params['alpha_1'],fitted_obj.params['sigma_1'],fitted_obj.params['meu_1'])\n",
    "gaus_2 = attribute_prob_obj.single_component(fitted_obj.params['alpha_2'],fitted_obj.params['sigma_2'],fitted_obj.params['meu_2'])\n",
    "plt.plot(attribute_prob_obj.x, yy,'r', label = 'best_fit')\n",
    "plt.plot(attribute_prob_obj.x, gaus_1,'g', label = 'gaus_1')\n",
    "plt.plot(attribute_prob_obj.x, gaus_2,'y', label = 'gaus_2')\n",
    "plt.xlabel('Average Cruise Speed (m/s)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(driving_pulse.avg_cruising_speed_inv_cdf_obj.x,driving_pulse.avg_cruising_speed_inv_cdf_obj.y,  label = 'minimized')\n",
    "plt.title(r'$f^{-1}(x)$')\n",
    "plt.xlabel('Uniformly distributed random number')\n",
    "plt.ylabel('Average Cruise Speed (m/s)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "#plot the histogram, each gaussian component and the combined gaussian curve\n",
    "plt.plot(extract_obj.idle_obj.bins,extract_obj.idle_obj.data_points,'b')\n",
    "\n",
    "#load initial gaussian parameters\n",
    "param_obj = Gaussian_idle_param()\n",
    "#create a probability function object for attribute with its attribute histogram data\n",
    "attribute_prob_obj = Probability_Functions(extract_obj.idle_obj.bins, extract_obj.idle_obj.data_points,2)\n",
    "#fit the histogram\n",
    "fitted_obj = attribute_prob_obj.NLLSR(param_obj)\n",
    "# # create inverse cdf object\n",
    "# inv_cdf_obj = inv_cdf(attribute_prob_obj)\n",
    "\n",
    "yy = attribute_prob_obj.eqn_model(fitted_obj.params)\n",
    "gaus_1 = attribute_prob_obj.single_component(fitted_obj.params['alpha_1'],fitted_obj.params['sigma_1'],fitted_obj.params['meu_1'])\n",
    "gaus_2 = attribute_prob_obj.single_component(fitted_obj.params['alpha_2'],fitted_obj.params['sigma_2'],fitted_obj.params['meu_2'])\n",
    "plt.plot(attribute_prob_obj.x, yy,'r', label = 'best_fit')\n",
    "plt.plot(attribute_prob_obj.x, gaus_1,'g', label = 'gaus_1')\n",
    "plt.plot(attribute_prob_obj.x, gaus_2,'y', label = 'gaus_2')\n",
    "plt.xlabel('Idle duration (s)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # loads the csv file\n",
    "    subdir = 'caltrans_processed_drive_cycles/data/1135317_2'\n",
    "    file_name = '2012-04-06.csv'\n",
    "    data = load_csv_data(file_name, subdir)\n",
    "    # get a slice of the data with a relatively long driving pulse\n",
    "    # data = data.iloc[883:1302,:]\n",
    "    # # show the slice of data\n",
    "    # plt.plot(data.loc[:,'timestamp'], data.loc[:,'speed_mph'])\n",
    "    # plt.show()\n",
    "    # print(data)\n",
    "    # get the slice of ONLY cruising period\n",
    "    cruising_data = data.iloc[915:1285,:]\n",
    "    print(cruising_data)\n",
    "    # create a numpy array of just t values starting at t=1\n",
    "    t = np.linspace(1,len(cruising_data),len(cruising_data))\n",
    "    print(t)\n",
    "    # create a numpy array of speed_mph values and convert to m/s\n",
    "    y = cruising_data.loc[:,'speed_mph'].to_numpy()/2.237\n",
    "    print(y)\n",
    "    # interpolate linearly and make timesteps finer (0.001s)\n",
    "    from scipy.interpolate import interp1d\n",
    "    f = interp1d(t, y)\n",
    "    t = np.linspace(1,len(cruising_data),1000*len(cruising_data))\n",
    "    y = f(t)\n",
    "\n",
    "    # initialise the DP object\n",
    "    vn_obj = Velocity_Noise(t,y)\n",
    "    # deduct the average from the cruising period speed values (from fig3a to fig3b) and store as y\n",
    "    y = vn_obj.subtract_avg()\n",
    "\n",
    "    original_y = y\n",
    "    \n",
    "    # perform NLLSR with the initial parameters suggested by LMParams\n",
    "    hi = vn_obj.fit_all()\n",
    "    yy = vn_obj.final_curve()\n",
    "    \n",
    "    driving_pulse = DP(pulse_duration=500)\n",
    "    \n",
    "    plt.plot(t,y,'b')\n",
    "    plt.plot(t, yy,'r', label = 'best_fit')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    driving_pulse.generate_drive_cycle(vn_obj)\n",
    "\n"
   ]
  },
  {
   "source": [
    "# The code below is for when we are balancing all 3 components of a Frequency Spectrum (FS) at ONCE\n",
    "## Each block corresponds to one FS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # loads the csv file\n",
    "    subdir = 'caltrans_processed_drive_cycles/data/1035198_1'\n",
    "    file_name = '2012-05-22.csv'\n",
    "    data = load_csv_data(file_name, subdir)\n",
    "    # get a slice of the data with a relatively long driving pulse\n",
    "    data = data.iloc[1002:1096,:]\n",
    "    # show the slice of data\n",
    "    plt.plot(data.loc[:,'timestamp'], data.loc[:,'speed_mph'])\n",
    "    plt.show()\n",
    "    # get the slice of ONLY cruising period\n",
    "    cruising_data = data.iloc[25:86,:]\n",
    "\n",
    "    # create a numpy array of just t values starting at t=1\n",
    "    t = np.linspace(1,len(cruising_data),len(cruising_data))\n",
    "    # create a numpy array of speed_mph values\n",
    "    y = cruising_data.loc[:,'speed_mph'].to_numpy()\n",
    "    # initialise the DP object\n",
    "    vn_obj = Velocity_Noise(t,y)\n",
    "    # deduct the average from the cruising period speed values (from fig3a to fig3b) and store as y\n",
    "    y = vn_obj.subtract_avg()\n",
    "\n",
    "    original_y = y\n",
    " \n",
    "    # perform NLLSR with the initial parameters suggested by LMParams\n",
    "    hi = vn_obj.LF_fit()\n",
    "    plt.plot(t,y,'b')\n",
    "\n",
    "    yy = hi.params['A1_FS'] * np.sin( (hi.params['w1_FS']*t) + hi.params['phi1_FS'])\n",
    "    yy = yy + hi.params['A2_FS'] * np.sin( (hi.params['w2_FS']*t) + hi.params['phi2_FS'])\n",
    "    yy = yy + hi.params['A3_FS'] * np.sin( (hi.params['w3_FS']*t) + hi.params['phi3_FS'])\n",
    "    plt.plot(t, yy,'r', label = 'best_fit')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    # remove previous component of LF spectrum\n",
    "    y = vn_obj.subtract(yy)\n",
    "    reconstructed = yy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # perform NLLSR with the initial parameters suggested by LMParams\n",
    "    hi2 = vn_obj.MF_fit()\n",
    "\n",
    "    plt.plot(t,y,'b')\n",
    "    yy = hi2.params['A1_FS'] * np.sin( (hi2.params['w1_FS']*t) + hi2.params['phi1_FS'])\n",
    "    yy = yy + hi2.params['A2_FS'] * np.sin( (hi2.params['w2_FS']*t) + hi2.params['phi2_FS'])\n",
    "    yy = yy + hi2.params['A3_FS'] * np.sin( (hi2.params['w3_FS']*t) + hi2.params['phi3_FS'])\n",
    "    plt.plot(t, yy,'r', label = 'best_fit')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    # remove previous component of LF spectrum\n",
    "    y = vn_obj.subtract(yy)\n",
    "    reconstructed += yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # perform NLLSR with the initial parameters suggested by LMParams\n",
    "    hi3 = vn_obj.HF_fit()\n",
    "    plt.plot(t,y,'b')\n",
    "    yy = hi3.params['A1_FS'] * np.sin( (hi3.params['w1_FS']*t) + hi3.params['phi1_FS'])\n",
    "    yy = yy + hi3.params['A2_FS'] * np.sin( (hi3.params['w2_FS']*t) + hi3.params['phi2_FS'])\n",
    "    yy = yy + hi3.params['A3_FS'] * np.sin( (hi3.params['w3_FS']*t) + hi3.params['phi3_FS'])\n",
    "    plt.plot(t, yy,'r', label = 'best_fit')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    # remove previous component of LF spectrum\n",
    "    y = vn_obj.subtract(yy)\n",
    "    reconstructed += yy\n",
    "\n",
    "\n",
    "    # create a numpy array of just new_t values starting at t=1\n",
    "    new_t = np.linspace(1,len(cruising_data),1000*len(cruising_data))\n",
    "    vn_obj.set_t(new_t)\n",
    "\n",
    "    plt.plot(t,original_y,'b')\n",
    "    plt.plot(new_t, vn_obj.final_curve(),'r', label = 'best_fit')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code below is for when we are running one component at once \n",
    "## Each block is for one Frequency Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # loads the csv file\n",
    "    subdir = 'caltrans_processed_drive_cycles/data/1035198_1'\n",
    "    file_name = '2012-05-22.csv'\n",
    "    data = load_csv_data(file_name, subdir)\n",
    "    # get a slice of the data with a relatively long driving pulse\n",
    "    data = data.iloc[1002:1096,:]\n",
    "    plt.plot(data.loc[:,'timestamp'], data.loc[:,'speed_mph'])\n",
    "    plt.show()\n",
    "    # get the slice of ONLY cruising period\n",
    "    cruising_data = data.iloc[25:86,:]\n",
    "\n",
    "    # create a numpy array of just t values starting at t=1\n",
    "    t = np.linspace(1,len(cruising_data),len(cruising_data))\n",
    "    # create a numpy array of speed_mph values\n",
    "    y = cruising_data.loc[:,'speed_mph'].to_numpy()\n",
    "    # initialise the DP object\n",
    "    vn_obj = Velocity_Noise(t,y)\n",
    "    # deduct the average from the cruising period speed values (from fig3a to fig3b) and store as y\n",
    "    y = vn_obj.subtract_avg()\n",
    "\n",
    "    original_y = y\n",
    "    \n",
    "    # perform NLLSR with the initial parameters suggested by LMParams\n",
    "    hi = vn_obj.NLLSR(LF_Noise(1))\n",
    "    plt.plot(t,y,'b')\n",
    "    yy = vn_obj.single_component(hi.params['A_FS'],hi.params['w_FS'],hi.params['phi_FS'])\n",
    "    plt.plot(t, yy,'r', label = 'best_fit')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    # remove previous component of LF spectrum\n",
    "    y = vn_obj.subtract(yy)\n",
    "    reconstructed = yy\n",
    "\n",
    "    # perform NLLSR with the initial parameters suggested by LMParams\n",
    "    hi = vn_obj.NLLSR(LF_Noise(2))\n",
    "    plt.plot(t,y,'b')\n",
    "    yy = vn_obj.single_component(hi.params['A_FS'],hi.params['w_FS'],hi.params['phi_FS'])\n",
    "    plt.plot(t, yy,'r', label = 'best_fit')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    # remove previous component of LF spectrum\n",
    "    y = vn_obj.subtract(yy)\n",
    "    reconstructed += yy\n",
    "\n",
    "    # perform NLLSR with the initial parameters suggested by LMParams\n",
    "    hi = vn_obj.NLLSR(LF_Noise(3))\n",
    "    plt.plot(t,y,'b')\n",
    "    yy = vn_obj.single_component(hi.params['A_FS'],hi.params['w_FS'],hi.params['phi_FS'])\n",
    "    plt.plot(t, yy,'r', label = 'best_fit')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    # remove previous component of LF spectrum\n",
    "    y = vn_obj.subtract(yy)\n",
    "    reconstructed += yy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # perform NLLSR with the initial parameters suggested by LMParams\n",
    "    hi = vn_obj.NLLSR(MF_Noise(1))\n",
    "    plt.plot(t,y,'b')\n",
    "    yy = vn_obj.single_component(hi.params['A_FS'],hi.params['w_FS'],hi.params['phi_FS'])\n",
    "    plt.plot(t, yy,'r', label = 'best_fit')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    # remove previous component of MF spectrum\n",
    "    y = vn_obj.subtract(yy)\n",
    "    reconstructed += yy\n",
    "\n",
    "    # perform NLLSR with the initial parameters suggested by LMParams\n",
    "    hi = vn_obj.NLLSR(MF_Noise(2))\n",
    "    plt.plot(t,y,'b')\n",
    "    yy = vn_obj.single_component(hi.params['A_FS'],hi.params['w_FS'],hi.params['phi_FS'])\n",
    "    plt.plot(t, yy,'r', label = 'best_fit')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    # remove previous component of MF spectrum\n",
    "    y = vn_obj.subtract(yy)\n",
    "    reconstructed += yy\n",
    "\n",
    "    # perform NLLSR with the initial parameters suggested by LMParams\n",
    "    hi = vn_obj.NLLSR(MF_Noise(3))\n",
    "    plt.plot(t,y,'b')\n",
    "    yy = vn_obj.single_component(hi.params['A_FS'],hi.params['w_FS'],hi.params['phi_FS'])\n",
    "    plt.plot(t, yy,'r', label = 'best_fit')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    # remove previous component of MF spectrum\n",
    "    y = vn_obj.subtract(yy)\n",
    "    reconstructed += yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # perform NLLSR with the initial parameters suggested by LMParams\n",
    "    hi = vn_obj.NLLSR(HF_Noise(1))\n",
    "    plt.plot(t,y,'b')\n",
    "    yy = vn_obj.single_component(hi.params['A_FS'],hi.params['w_FS'],hi.params['phi_FS'])\n",
    "    plt.plot(t, yy,'r', label = 'best_fit')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    # remove previous component of HF spectrum\n",
    "    y = vn_obj.subtract(yy)\n",
    "    reconstructed += yy\n",
    "\n",
    "    # perform NLLSR with the initial parameters suggested by LMParams\n",
    "    hi = vn_obj.NLLSR(HF_Noise(2))\n",
    "    plt.plot(t,y,'b')\n",
    "    yy = vn_obj.single_component(hi.params['A_FS'],hi.params['w_FS'],hi.params['phi_FS'])\n",
    "    plt.plot(t, yy,'r', label = 'best_fit')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    # remove previous component of HF spectrum\n",
    "    y = vn_obj.subtract(yy)\n",
    "    reconstructed += yy\n",
    "\n",
    "    # perform NLLSR with the initial parameters suggested by LMParams\n",
    "    hi = vn_obj.NLLSR(HF_Noise(3))\n",
    "    plt.plot(t,y,'b')\n",
    "    yy = vn_obj.single_component(hi.params['A_FS'],hi.params['w_FS'],hi.params['phi_FS'])\n",
    "    plt.plot(t, yy,'r', label = 'best_fit')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    # remove previous component of HF spectrum\n",
    "    y = vn_obj.subtract(yy)\n",
    "    reconstructed += yy\n",
    "\n",
    "    plt.plot(t,original_y,'b')\n",
    "    plt.plot(t, reconstructed,'r', label = 'best_fit')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}